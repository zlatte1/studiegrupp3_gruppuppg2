{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf361c36",
   "metadata": {},
   "source": [
    "# Gruppuppgift Del 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b53cd1",
   "metadata": {},
   "source": [
    "**Name:**\n",
    "\n",
    "**Name of the person who graded you during \"kamratbedömning\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4370605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "import scipy.special \n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c5049",
   "metadata": {},
   "source": [
    "# Chapter 6 in the Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77511",
   "metadata": {},
   "source": [
    "## Create two empty vectors, x1 and x2. Now do a for loop 100 times, where you for each iteration: \n",
    "## 1) Take the mean of 100 samples from $N(10, 5)$ and store it in x1. \n",
    "## 2) Take the mean of 100 samples from $N(10, 20)$ and store it in x2.\n",
    "\n",
    "## So, your vectors x1 and x2 should contain 100 values. \n",
    "\n",
    "## From p.148 in the book, we know that the mean is an unbiased estimate of the fixed, but unknown $\\mu$. The nice thing is that in our Monte Carlo Simulation we know the true $\\mu$. Plot histograms of x1 and x2 and comment on the result, do we on \"average\" get the right mean? Is it easier or harder to estimate the mean when the variance is low/high? How is this knowledge useful in reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two empty list vector\n",
    "x1 = []\n",
    "x2 = []\n",
    "\n",
    "# Looping through each iteration\n",
    "for i in range(100):\n",
    "    rand = np.random.normal(10, 5, 100) # storing and using np.rand.normal function to generate 100 samples from the normal distribution of the given standard deviation and sigma\n",
    "    x = rand.mean() # getting the means of the distrubtions by using np.mean funtion \n",
    "    x1.append(x) # appanding the results to the x1 list\n",
    "\n",
    "x1 = np.array(x1) # converting the x1 into an np array\n",
    "    \n",
    "## once we write for X1, the procedure for x2 is completely similar    \n",
    "    \n",
    "# Looping through each iteration\n",
    "for i in range(100):\n",
    "    rand = np.random.normal(10, 20, 100)\n",
    "    x = rand.mean()\n",
    "    x2.append(x)\n",
    "\n",
    "x2 = np.array(x2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots() # initializing the matplotlib figure and axis.\n",
    "ax.hist(x1, label = 'x1', histtype = 'step') # making histogram of x1 variable, also add a label \n",
    "ax.hist(x2, label = 'x2', histtype = 'step') # 'step' makes the graph transperrant so if the two distribution overlapps, we will still see both of them.\n",
    "ax.legend() # displaying the variables on the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2417bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "x2 = []\n",
    "\n",
    "for i in range(1000):\n",
    "    rand = np.random.normal(10,5,100)\n",
    "    x = rand.mean()\n",
    "    x1.append(x)\n",
    "    \n",
    "x1 = np.array(x1)\n",
    "\n",
    "for i in range(1000):\n",
    "    rand = np.random.normal(10,20,100)\n",
    "    x = rand.mean()\n",
    "    x2.append(x)\n",
    "    \n",
    "x2 = np.array(x2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x1, label = 'x1', histtype = 'step')\n",
    "ax.hist(x2, label = 'x2', histtype = 'step')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a88c70",
   "metadata": {},
   "source": [
    "1 a-) do we on \"average\" get the right mean? \n",
    "\n",
    "Answer: Yes, by analyzing the histogram of mean of x1 and x2 for 100 iterations, its pretty clear that the peak of histogram for both variables is at 10, which was true mean of the normal distribution from which we sampled our random variable. However, the histogram for x2 is more spread out then x1. This is because x2 was sampled from normal distribution with higher standard deviation of 20 then x1 which had standard deviation of only 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffec8a",
   "metadata": {},
   "source": [
    "1 b-) Is it easier or harder to estimate the mean when the variance is low/high? \n",
    "\n",
    "Answer: Yes, estimating the mean of sampling distribution that have higher variance is sligtly difficult, as we can see x2 have major density populated between 9 and 10, whereas for x1 the peak is very clearly visible at 10 only. Hence, to estimate the mean of distribution with larger variance the sample size and number of samples needs to be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98e35",
   "metadata": {},
   "source": [
    "1 c-) How is this knowledge useful in reality?\n",
    "\n",
    "Answer: This knowledge is useful in reality to estimate lot of population parameters. For example in a production line of chips factory, production engineer or quality checker cannot wait each and every packet to determing the mean weight of all the packets. Rather, it would be handy to pick up few hundred samples of appropriate size to determine the true mean of packets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460de478",
   "metadata": {},
   "source": [
    "## Redo the same analysis as above but with 10000 generated samples and see if there is any difference (copy the code from above). Comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bde413",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "x2 = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand = np.random.normal(10,5,100)\n",
    "    x = rand.mean()\n",
    "    x1.append(x)\n",
    "    \n",
    "x1 = np.array(x1)\n",
    "\n",
    "for i in range(10000):\n",
    "    rand = np.random.normal(10,20,100)\n",
    "    x = rand.mean()\n",
    "    x2.append(x)\n",
    "    \n",
    "x2 = np.array(x2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x1, label = 'x1', histtype = 'step')\n",
    "ax.hist(x2, label = 'x2', histtype = 'step')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404dae53",
   "metadata": {},
   "source": [
    "2 -) Comment on the results.\n",
    "\n",
    "Comments: No, there is not much difference that is observed. The sampling distribution for x2 is still more spread out than x1. x1 has major density at around 10, whereas x2 has it between 8 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b8e50",
   "metadata": {},
   "source": [
    "# Chapter 7 - Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efa844",
   "metadata": {},
   "source": [
    "Assuming $X$ is $N(10, 3)$, we know (see example calculation below) that a random sample x will with probability 95% be between: <br>\n",
    "ci_lower = 10 - 1.96 x 3 <br> and <br>\n",
    "ci_upper = 10 + 1.96 x 3\n",
    "\n",
    "Example, for the ci_upper, we do the following calculation: $P(X > ci\\_upper) = P(X > 10 + 1.96 * 3) = P(Z > 1.96) = 0.025$ where we used the usual standardization. \n",
    "\n",
    "This means that if you get observations from the normal distribution above, you would \"expect\" 95 \\% of them to be between ci_lower and ci_upper. \n",
    "\n",
    "Your task is now to simulate 10, 100, 1000 and 10000 normally distributed samples from $N(10, 3)$ and check the relative frequency (\\%) of how many samples fall outside the confidence interval. How many \\% should it theoretically be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_lower = 10 - 1.96*3\n",
    "ci_upper = 10 + 1.96*3\n",
    "\n",
    "print(ci_lower)\n",
    "print(ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"part 1\"\"\"\n",
    "list_10 = np.random.normal(10, 3, size=10)\n",
    "list_100 = np.random.normal(10, 3, size=100)\n",
    "list_1000 = np.random.normal(10, 3, size=1000)\n",
    "list_10000 = np.random.normal(10, 3, size=10000)\n",
    "\n",
    "print(list_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"testing\"\"\"\n",
    "\n",
    "test_list = np.array([1, 12, 12, 12, 12, 12, 16, 1, 15, 15])\n",
    "\n",
    "#skapar true/false = binomial\n",
    "rel_test_list = np.array((ci_lower < test_list) & (test_list < ci_upper))\n",
    "\n",
    "#samma som ovan + summering av de värden som ligger i utfallet\n",
    "rel_test = np.array(((ci_lower > test_list) + (test_list > ci_upper)).sum())\n",
    "\n",
    "print('som binomial: ', rel_test_list)\n",
    "print()\n",
    "print('antal false = dvs utanför 95% =', rel_test)\n",
    "print()\n",
    "print(type(rel_test)) # var tvungen att vara en array för att ovan skulle fungera.\n",
    "print(len(test_list)) # för att kolla så att \n",
    "\n",
    "print()\n",
    "print('test', (rel_test/len(test_list))*100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5519ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"part 2 = rel freq in % \"\"\"\n",
    "rel_10 = ((ci_lower > list_10) + (list_10 > ci_upper)).sum()\n",
    "rel_100 = ((ci_lower > list_100) + (list_100 > ci_upper)).sum()\n",
    "rel_1000 = ((ci_lower > list_1000) + (list_1000 > ci_upper)).sum()\n",
    "rel_10000 = ((ci_lower > list_10000) + (list_10000 > ci_upper)).sum()\n",
    "\n",
    "print((rel_10/len(list_10))*100, '%')\n",
    "print((rel_100/len(list_100))*100, '%')\n",
    "print((rel_1000/len(list_1000))*100, '%')\n",
    "print((rel_10000/len(list_10000))*100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f5b4b",
   "metadata": {},
   "source": [
    "### Theoretical should be 5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894a103",
   "metadata": {},
   "source": [
    "google sökningar med resultat som hjälpt till resultatet\n",
    "\n",
    "https://stackoverflow.com/questions/9560207/how-to-count-values-in-a-certain-range-in-a-numpy-array\n",
    "\n",
    "https://www.askpython.com/python/array/array-length-in-python\n",
    "\n",
    "https://www.datacamp.com/tutorial/python-arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf91ab1",
   "metadata": {},
   "source": [
    "## Assume you have 100 observations from N(10, 3), you calculate a confidence interval and check if it \"captures\" the real $\\mu$ (we know the reality since we are doing a monte carlo simulation), how often do you \"capture\" the real mu (in \\%) when you repeat this 1000 times? How is this knowledge useful in reality?\n",
    "\n",
    "## Note, the difference between this question and the one above is that here we calculate the sample mean and sample standard deviation to construct confidence intervals whereas above we used that we \"knew the reality\" ($\\mu$ and $\\sigma$), i.e. ci_lower <- 10 - 1.96*3 and  ci_upper <- 10 + 1.96*3. Obviosuly, this is not the case in reality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d52956",
   "metadata": {},
   "source": [
    "Part I = one Monte Carlo sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo = np.random.normal(10, 3, size=100)\n",
    "\n",
    "m = monte_carlo.mean() \n",
    "s = monte_carlo.std() \n",
    "dof = len(monte_carlo)-1 \n",
    "confidence = 0.95\n",
    "\n",
    "# np.abs - för att få endast positiva värden\n",
    "#t kommer från scipy.stats\n",
    "t_crit = np.abs(scipy.stats.t.ppf((1-confidence)/2, dof))\n",
    "\n",
    "lower_ci = m-s*t_crit/np.sqrt(len(monte_carlo))\n",
    "upper_ci = m+s*t_crit/np.sqrt(len(monte_carlo))\n",
    "\n",
    "print('With', confidence*100, '% confidence the real mu is between', lower_ci, 'and', upper_ci )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822dc2c",
   "metadata": {},
   "source": [
    "Part II = the % of trails that is within 95% confidence out of \"n\" (1000) trails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aa415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rel_freq_monte = []\n",
    "dof = 100-1 \n",
    "dof_2 = 1000-1\n",
    "confidence = 0.95\n",
    "\n",
    "trials = 1000\n",
    "\n",
    "t_crit = np.abs(scipy.stats.t.ppf((1-confidence)/2, dof))\n",
    "t_crit_2 = np.abs(scipy.stats.t.ppf((1-confidence)/2, dof_2))\n",
    "\n",
    "for i in range(trials):\n",
    "    monte = np.random.normal(10, 3, size=100)\n",
    "    monte_mean = np.mean(monte)\n",
    "    monte_std = np.std(monte)\n",
    "    lower_ci = (monte_mean-monte_std*t_crit/np.sqrt(trials))\n",
    "    upper_ci = (monte_mean+monte_std*t_crit/np.sqrt(trials))\n",
    "    cap_monte = ((lower_ci > monte) + (monte > upper_ci)).sum()\n",
    "    rel_freq_monte.append(cap_monte)\n",
    "\n",
    "n_trails_mean = np.mean(rel_freq_monte)\n",
    "n_trails_std = np.std(rel_freq_monte)\n",
    "    \n",
    "n_trials_low = (n_trails_mean-n_trails_std*t_crit_2/np.sqrt(len(rel_freq_monte))) \n",
    "n_trials_up = (n_trails_mean+n_trails_std*t_crit_2/np.sqrt(len(rel_freq_monte)))\n",
    "n_trials_inside = ((n_trials_low > rel_freq_monte) + (rel_freq_monte > n_trials_up)).sum()  \n",
    "\n",
    "print('% av antalet trails som är innanför 95 % ',(n_trials_inside/trials)*100)\n",
    "\n",
    "# print(monte)\n",
    "# print()\n",
    "# print(monte_mean)\n",
    "# print()\n",
    "# print(monte_std)\n",
    "# print()\n",
    "# print(rel_freq_monte)\n",
    "# # print(cap_monte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e094ef",
   "metadata": {},
   "source": [
    "## Confidence Intervals for proportions. \n",
    "So far, we have looked at confidence intervals for $\\mu$. Often, we are also interested in proportions, $\\pi$, this is for instance the case in elections that get much attention at TV and \"nyhetsmorgon\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dee8d6",
   "metadata": {},
   "source": [
    "## Assume we ask 1000 people who they will vote for and 4.2% answer Centerpartiet (C). Create a 90% Confidence Interval to get the proportion that would vote for C if we surveyed the whole population (\"Totalundersökning\"). OBS: note we want a 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to calculate CI when we know:\n",
    "#population and favourable_outcome || missing: standardDIV, x_bar\n",
    "# N = number of samplesize\n",
    "# x = favourable_outcome\n",
    "# CL = Confidencelevel in fraction\n",
    "# Z = from z-table; 95% = 1.9600, 90% = 1.6449\n",
    "# return is a tuple (lower_ci, upper_ci)\n",
    "# Calulator to check that the function works as expected || https://sample-size.net/confidence-interval-proportion/ \n",
    "\n",
    "import numpy as np\n",
    "def calc_ci(N, x, CL, Z):\n",
    "    SEM = np.sqrt((x*(N-x)/N**3))\n",
    "    PPR = x/N\n",
    "    return ((PPR-(Z*SEM)),(PPR+(Z*SEM)))\n",
    "\n",
    "lower_ci, upper_ci = calc_ci(1000, 42, 0.90, 1.6449)\n",
    "print(lower_ci)\n",
    "print(upper_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found a shorter function for this. \n",
    "# count = favourable_outcome\n",
    "# nobs = number of samplesize\n",
    "# alpha = deviation from standard [standard beeing 95%, if you want anything else, type the difference in fraction]\n",
    "# return is a tuple (lower_ci, upper_ci)\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "lower_ci, upper_ci = proportion_confint(count=42, nobs=1000, alpha=0.10)\n",
    "print(lower_ci)\n",
    "print(upper_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f039ee",
   "metadata": {},
   "source": [
    "## For this question see (p. 177 - 178) in the book. We ask 1000 people who they will vote for, 4.2% answer Centerpartiet (C) and 4.5% answer Liberalerna (L), is there a statistical significant difference between the two parties? \n",
    "\n",
    "## Check if the assumption on  np(1-p) > 5 is satisfied for both poportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efbdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Significant difference\n",
    "# p1 = number of favourable_outcome sample1\n",
    "# n1 = number of samplesize sample1\n",
    "# p2 = number of favourable_outcome sample2\n",
    "# n2 = number of samplesize sample2\n",
    "# return is a boolean \n",
    "# Z = from z-table, 95% CL = 1.96, 90% = 1.6449\n",
    "# source | https://help.surveymonkey.com/en/analyze/significant-differences/\n",
    "\n",
    "import numpy as np\n",
    "def calc_significant_diff(p1, n1, p2, n2, Z):\n",
    "    a1 = p1/n1*n1\n",
    "    b1 = p2/n2*n2\n",
    "    p = (p1+p2) / (n1+n2)\n",
    "    SE = np.sqrt((p*(1-p)) * ((1/n1) +(1/n2)))\n",
    "    t = (p1/n1-p2/n2) / SE\n",
    "    if(t > Z):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "calc_significant_diff(42, 1000, 45, 1000, 1.6449)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33358ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6d39ad",
   "metadata": {},
   "source": [
    "# Chapter 8 - Hypothesis Testing\n",
    "In the code below, can we reject the hypothesis that $\\mu = 5$ ? Can we reject the hypothesis that $\\mu = 0$ ? How do you interpret the p-value?\n",
    "\n",
    "Read the following documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html to understand the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0293b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scipy.stats.norm.rvs(loc = 0, scale = 4, size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762425cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scipy.stats.ttest_1samp(x, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a407c",
   "metadata": {},
   "source": [
    "Answer: Reject, the p value is far far below 0.05.\n",
    "(as the 1.7369.... is to the power of e-22. i.e 0.000000......17369)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b2c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scipy.stats.ttest_1samp(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7bd99",
   "metadata": {},
   "source": [
    "Answer: Accept, the p value is greater than 0.05\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
